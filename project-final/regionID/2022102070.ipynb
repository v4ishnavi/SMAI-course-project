{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":99359,"databundleVersionId":11870477,"sourceType":"competition"},{"sourceId":374909,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":309918,"modelId":330287}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"labels_train = '/kaggle/input/smai-25-sec-a-project-phase-2-region-id-prediction/labels_train.csv'\nlabels_val = '/kaggle/input/smai-25-sec-a-project-phase-2-region-id-prediction/labels_val.csv'\n\nimage_train_folder = '/kaggle/input/smai-25-sec-a-project-phase-2-region-id-prediction/images_train/images_train'\nimage_val_folder = '/kaggle/input/smai-25-sec-a-project-phase-2-region-id-prediction/images_val/images_val'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T00:04:28.401362Z","iopub.execute_input":"2025-05-06T00:04:28.401680Z","iopub.status.idle":"2025-05-06T00:04:28.409746Z","shell.execute_reply.started":"2025-05-06T00:04:28.401656Z","shell.execute_reply":"2025-05-06T00:04:28.408785Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ndf = pd.read_csv(labels_train)\n# df['timestamp'] = pd.to_datetime(df['timestamp'], format='%H:%M').dt.time\n\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T00:04:30.171554Z","iopub.execute_input":"2025-05-06T00:04:30.172495Z","iopub.status.idle":"2025-05-06T00:04:30.545124Z","shell.execute_reply.started":"2025-05-06T00:04:30.172461Z","shell.execute_reply":"2025-05-06T00:04:30.544071Z"}},"outputs":[{"name":"stdout","text":"       filename timestamp  latitude  longitude  angle  Region_ID\n0  img_0000.jpg     15:03    219698     144782    133          2\n1  img_0001.jpg     15:05    219844     144621    312          2\n2  img_0002.jpg     15:05    219844     144621    359          2\n3  img_0003.jpg     17:11    219514     145016    131          2\n4  img_0004.jpg     17:00    220182     144211     45          2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\nimport cv2\nimport os\nimage = cv2.imread('/kaggle/input/smai-25-sec-a-project-phase-2-region-id-prediction/images_train/images_train/img_0000.jpg')\nprint(image.shape)\n\n# in the df change all the file names to full path \ndf['filename'] = df['filename'].apply(lambda x: os.path.join('/kaggle/input/smai-25-sec-a-project-phase-2-region-id-prediction/images_train/images_train/', x))\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T00:04:32.288895Z","iopub.execute_input":"2025-05-06T00:04:32.289790Z","iopub.status.idle":"2025-05-06T00:04:32.757186Z","shell.execute_reply.started":"2025-05-06T00:04:32.289751Z","shell.execute_reply":"2025-05-06T00:04:32.756455Z"}},"outputs":[{"name":"stdout","text":"(256, 256, 3)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                            filename timestamp  latitude  \\\n0  /kaggle/input/smai-25-sec-a-project-phase-2-re...     15:03    219698   \n1  /kaggle/input/smai-25-sec-a-project-phase-2-re...     15:05    219844   \n2  /kaggle/input/smai-25-sec-a-project-phase-2-re...     15:05    219844   \n3  /kaggle/input/smai-25-sec-a-project-phase-2-re...     17:11    219514   \n4  /kaggle/input/smai-25-sec-a-project-phase-2-re...     17:00    220182   \n\n   longitude  angle  Region_ID  \n0     144782    133          2  \n1     144621    312          2  \n2     144621    359          2  \n3     145016    131          2  \n4     144211     45          2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>timestamp</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>angle</th>\n      <th>Region_ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/smai-25-sec-a-project-phase-2-re...</td>\n      <td>15:03</td>\n      <td>219698</td>\n      <td>144782</td>\n      <td>133</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/smai-25-sec-a-project-phase-2-re...</td>\n      <td>15:05</td>\n      <td>219844</td>\n      <td>144621</td>\n      <td>312</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/smai-25-sec-a-project-phase-2-re...</td>\n      <td>15:05</td>\n      <td>219844</td>\n      <td>144621</td>\n      <td>359</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/smai-25-sec-a-project-phase-2-re...</td>\n      <td>17:11</td>\n      <td>219514</td>\n      <td>145016</td>\n      <td>131</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/smai-25-sec-a-project-phase-2-re...</td>\n      <td>17:00</td>\n      <td>220182</td>\n      <td>144211</td>\n      <td>45</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from torchvision import transforms\n\ntrain_transforms = transforms.Compose([\n    # transforms.RandomResizedCrop(256, scale=(0.8,1.0)),\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n\n    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n])\n\ntrain_transforms2 = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n    transforms.ToTensor(),\n    \n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T00:04:35.576146Z","iopub.execute_input":"2025-05-06T00:04:35.576467Z","iopub.status.idle":"2025-05-06T00:04:43.871119Z","shell.execute_reply.started":"2025-05-06T00:04:35.576443Z","shell.execute_reply":"2025-05-06T00:04:43.870421Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\n\nclass MyDataset(Dataset):\n    def __init__(self, df, transform=None, validation = False, label_fixing = True):\n        self.df = df\n        self.transform = transform\n        self.double_len = len(df) * 2\n        self.validation = validation\n        if label_fixing:\n            self.df['Region_ID'] = self.df['Region_ID'] - 1\n\n    def __len__(self):\n        if self.validation:\n            return len(self.df)\n        else:\n            return self.double_len\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx % len(self.df)]\n        img = Image.open(f\"{row.filename}\").convert('RGB')\n        img = img.resize((224, 224))\n        if self.validation:\n            transformed_img = transforms.ToTensor()(img)\n        elif idx < len(self.df):\n            transformed_img = transforms.ToTensor()(img)\n        else:\n            transformed_img = self.transform(img) if self.transform else img\n\n        label = row.Region_ID\n        return transformed_img, label\n\ntrain_dataset = MyDataset(df, transform=train_transforms) \nprint(len(train_dataset))\nimg_sample, label = train_dataset[len(df) + 4]\nprint(img_sample.shape)\nprint(label)\n\nvalidation_df = pd.read_csv(labels_val)\nvalidation_df['filename'] = validation_df['filename'].apply(lambda x: os.path.join('/kaggle/input/smai-25-sec-a-project-phase-2-region-id-prediction/images_val/images_val/', x))\nvalidation_dataset = MyDataset(validation_df, transform=train_transforms, validation=True)\n\nprint(len(validation_dataset))\nimg_sample, label = validation_dataset[0]\nprint(img_sample.shape)\nprint(label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T00:04:43.872308Z","iopub.execute_input":"2025-05-06T00:04:43.873286Z","iopub.status.idle":"2025-05-06T00:04:44.044622Z","shell.execute_reply.started":"2025-05-06T00:04:43.873261Z","shell.execute_reply":"2025-05-06T00:04:44.043895Z"}},"outputs":[{"name":"stdout","text":"13084\ntorch.Size([3, 224, 224])\n1\n369\ntorch.Size([3, 224, 224])\n1\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T00:04:49.301106Z","iopub.execute_input":"2025-05-06T00:04:49.301412Z","iopub.status.idle":"2025-05-06T00:04:49.305482Z","shell.execute_reply.started":"2025-05-06T00:04:49.301389Z","shell.execute_reply":"2025-05-06T00:04:49.304638Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=16, shuffle=False)\n# device = \nprint(len(train_loader))\nprint(len(validation_loader))\n# print(device)\nprint(train_loader.dataset[0][0].shape)\nsample_image, sample_label = train_loader.dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T00:04:52.880086Z","iopub.execute_input":"2025-05-06T00:04:52.880385Z","iopub.status.idle":"2025-05-06T00:04:52.896400Z","shell.execute_reply.started":"2025-05-06T00:04:52.880363Z","shell.execute_reply":"2025-05-06T00:04:52.895202Z"}},"outputs":[{"name":"stdout","text":"818\n24\ntorch.Size([3, 224, 224])\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T00:04:56.915355Z","iopub.execute_input":"2025-05-06T00:04:56.915781Z","iopub.status.idle":"2025-05-06T00:04:56.920727Z","shell.execute_reply.started":"2025-05-06T00:04:56.915752Z","shell.execute_reply":"2025-05-06T00:04:56.919727Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\n\n\nimport wandb\nwandb.login(key = secret_value_0)\n# wandb.init(\n#     project=\"smai-project-regionID-classification\",\n#     name=\"vit-base-224-finetune\",\n#     config={\n#         \"epochs\": 40,\n#         \"batch_size\": train_loader.batch_size,\n#         \"lr\": 5e-5,\n#         \"weight_decay\": 0.01,\n#         \"model\": \"ViT-base-patch16-224\",\n#         \"num_labels\": 15\n#     }\n# )\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T00:04:58.914027Z","iopub.execute_input":"2025-05-06T00:04:58.914363Z","iopub.status.idle":"2025-05-06T00:05:09.009995Z","shell.execute_reply.started":"2025-05-06T00:04:58.914336Z","shell.execute_reply":"2025-05-06T00:05:09.009266Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mv4ishnavi\u001b[0m (\u001b[33mv4ishnavi-iiit-hyderabad\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# # fully finetuning a vit v\n# import torch\n# from transformers import ViTForImageClassification\n\n# num_labels = 15  \n# model = ViTForImageClassification.from_pretrained(\n#     'google/vit-base-patch16-224-in21k',\n#     num_labels=num_labels\n# )\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# model.to(device)\n# # Watch model gradients and parameter updates\n# wandb.watch(model, log=\"all\", log_freq=100)\n# from torch import nn, optim\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-06T00:03:45.204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import copy\n\n# class EarlyStopping:\n#     def __init__(self, patience=5, min_delta=0.0, verbose=False):\n#         self.patience = patience\n#         self.min_delta = min_delta\n#         self.verbose = verbose\n#         self.counter = 0\n#         self.best_loss = float('inf')\n#         self.best_model_state = None\n#         self.early_stop = False\n\n#     def __call__(self, val_loss, model):\n#         if val_loss < self.best_loss - self.min_delta:\n#             self.best_loss = val_loss\n#             self.best_model_state = copy.deepcopy(model.state_dict())\n#             self.counter = 0\n#             if self.verbose:\n#                 print(f\"Validation loss improved to {val_loss:.4f}.\")\n#         else:\n#             self.counter += 1\n#             if self.verbose:\n#                 print(f\"No improvement in validation loss for {self.counter} epoch(s).\")\n#             if self.counter >= self.patience:\n#                 self.early_stop = True\n#                 if self.verbose:\n#                     print(\"Early stopping triggered.\")\n\n# num_epochs = 40","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-06T00:03:45.204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Initialize early stopping\n# early_stopping = EarlyStopping(patience=3, min_delta=0.001, verbose=True)\n# from tqdm import tqdm \n# for epoch in range(num_epochs):\n#     # Training phase\n#     model.train()\n#     train_loss = 0.0\n#     correct_train = 0\n#     total_train = 0\n\n#     for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n#         inputs = images.to(device)\n#         labels = labels.to(device)\n\n#         optimizer.zero_grad()\n#         outputs = model(pixel_values=inputs)\n#         loss = criterion(outputs.logits, labels)\n#         loss.backward()\n#         optimizer.step()\n\n#         train_loss += loss.item() * inputs.size(0)\n#         _, predicted = outputs.logits.max(1)\n#         correct_train += predicted.eq(labels).sum().item()\n#         total_train += labels.size(0)\n\n#     avg_train_loss = train_loss / total_train\n#     train_accuracy = correct_train / total_train\n\n#     # Evaluation phase\n#     model.eval()\n#     val_loss = 0.0\n#     correct_val = 0\n#     total_val = 0\n\n#     with torch.no_grad():\n#         for images, labels in tqdm(validation_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n#             inputs = images.to(device)\n#             labels = labels.to(device)\n\n#             outputs = model(pixel_values=inputs)\n#             loss = criterion(outputs.logits, labels)\n\n#             val_loss += loss.item() * inputs.size(0)\n#             _, predicted = outputs.logits.max(1)\n#             correct_val += predicted.eq(labels).sum().item()\n#             total_val += labels.size(0)\n\n#     avg_val_loss = val_loss / total_val\n#     val_accuracy = correct_val / total_val\n\n#     print(f\"Epoch {epoch+1}/{num_epochs}\")\n#     print(f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n#     print(f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n#     wandb.log({\n#         \"epoch\": epoch + 1,\n#         \"train_loss\": avg_train_loss,\n#         \"train_accuracy\": train_accuracy,\n#         \"val_loss\": avg_val_loss,\n#         \"val_accuracy\": val_accuracy\n#     })\n\n#     # Check early stopping condition\n#     early_stopping(avg_val_loss, model)\n\n#     if early_stopping.early_stop:\n#         print(\"Early stopping triggered. Restoring best model weights.\")\n#         model.load_state_dict(early_stopping.best_model_state)\n#         break\n# torch.save(model.state_dict(), \"vit_finetuned.pth\")\n# wandb.save(\"vit_finetuned.pth\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-06T00:03:45.205Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"trying yolo ...","metadata":{}},{"cell_type":"code","source":"# for yolo we will have to restructure the files \nimport os\nimport shutil\nimport pandas as pd\nwandb.init(\n    project=\"smai-project-regionID-classification\",\n    name=\"yolov11-v2-x\",\n    config={\n        \"epochs\": 50,\n        \"batch_size\": 32,\n        \"model\": \"yolov11x\",\n        \"num_labels\": 15\n    }\n)\ndf['Region_ID'] = df['Region_ID'].astype(str)\nvalidation_df['Region_ID'] = validation_df['Region_ID'].astype(str)\n\nbase_dir = \"/kaggle/working/yolo_dataset\"\nall_labels = sorted(set(df['Region_ID'].unique()) | set(validation_df['Region_ID'].unique()))\nall_labels_str = [str(l) for l in all_labels]\n\nfor split_df, split in [(df, 'train'), (validation_df, 'val')]:\n    for label in all_labels_str:\n        os.makedirs(os.path.join(base_dir, split, label), exist_ok=True)\n\ndef copy_images(split_df, split_name):\n    for _, row in split_df.iterrows():\n        src = row['filename']\n        label = row['Region_ID']\n        dst = os.path.join(base_dir, split_name, label, os.path.basename(src))\n        shutil.copy(src, dst)\n\ncopy_images(df, 'train')\ncopy_images(validation_df, 'val')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T00:05:19.537486Z","iopub.execute_input":"2025-05-06T00:05:19.538248Z","iopub.status.idle":"2025-05-06T00:06:25.013634Z","shell.execute_reply.started":"2025-05-06T00:05:19.538217Z","shell.execute_reply":"2025-05-06T00:06:25.012623Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250506_000519-ntumcin1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/v4ishnavi-iiit-hyderabad/smai-project-regionID-classification/runs/ntumcin1' target=\"_blank\">yolov11-v2-x</a></strong> to <a href='https://wandb.ai/v4ishnavi-iiit-hyderabad/smai-project-regionID-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/v4ishnavi-iiit-hyderabad/smai-project-regionID-classification' target=\"_blank\">https://wandb.ai/v4ishnavi-iiit-hyderabad/smai-project-regionID-classification</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/v4ishnavi-iiit-hyderabad/smai-project-regionID-classification/runs/ntumcin1' target=\"_blank\">https://wandb.ai/v4ishnavi-iiit-hyderabad/smai-project-regionID-classification/runs/ntumcin1</a>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T00:06:25.015070Z","iopub.execute_input":"2025-05-06T00:06:25.015341Z","iopub.status.idle":"2025-05-06T00:07:52.902510Z","shell.execute_reply.started":"2025-05-06T00:06:25.015319Z","shell.execute_reply":"2025-05-06T00:07:52.901685Z"}},"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.3.127-py3-none-any.whl.metadata (37 kB)\nRequirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.2)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.5.1+cu124)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.3)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.12.2)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.0->ultralytics) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nDownloading ultralytics-8.3.127-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.127 ultralytics-thop-2.0.14\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import yaml\ndata_yaml = {\n    'path': base_dir,\n    'train': 'train',\n    'val': 'val',\n    'nc': len(all_labels_str),\n    'names': all_labels_str\n}\n\nwith open(os.path.join(base_dir, 'data.yaml'), 'w') as f:\n    yaml.dump(data_yaml, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T00:07:52.903847Z","iopub.execute_input":"2025-05-06T00:07:52.904216Z","iopub.status.idle":"2025-05-06T00:07:52.912296Z","shell.execute_reply.started":"2025-05-06T00:07:52.904186Z","shell.execute_reply":"2025-05-06T00:07:52.911465Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"!cat /kaggle/working/yolo_dataset/data.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T00:07:52.913296Z","iopub.execute_input":"2025-05-06T00:07:52.913607Z","iopub.status.idle":"2025-05-06T00:07:53.065652Z","shell.execute_reply.started":"2025-05-06T00:07:52.913560Z","shell.execute_reply":"2025-05-06T00:07:53.064662Z"}},"outputs":[{"name":"stdout","text":"names:\n- '0'\n- '1'\n- '10'\n- '11'\n- '12'\n- '13'\n- '14'\n- '2'\n- '3'\n- '4'\n- '5'\n- '6'\n- '7'\n- '8'\n- '9'\nnc: 15\npath: /kaggle/working/yolo_dataset\ntrain: train\nval: val\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from ultralytics import YOLO, settings\nsettings.update({'wandb': True}) \n\nos.environ['WANDB_PROJECT'] = 'smai-project-regionID-classification'\nos.environ['WANDB_NAME'] = 'yolo11m-bigger-classification-run'\nmodel = YOLO('yolo11x-cls.pt')  \ndataset_path = '/kaggle/working/yolo_dataset'\nmodel.train(\n    data=dataset_path,\n    epochs=50,\n    imgsz=256,\n    batch=32,\n    patience=10\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T00:07:53.068102Z","iopub.execute_input":"2025-05-06T00:07:53.068466Z","execution_failed":"2025-05-06T00:09:14.237Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-cls.pt to 'yolo11x-cls.pt'...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 56.9M/56.9M [00:02<00:00, 27.8MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Ultralytics 8.3.127 🚀 Python-3.11.11 torch-2.5.1+cu124 CPU (Intel Xeon 2.20GHz)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/working/yolo_dataset, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=256, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11x-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/classify/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n\u001b[34m\u001b[1mtrain:\u001b[0m /kaggle/working/yolo_dataset/train... found 6542 images in 15 classes ✅ \n\u001b[34m\u001b[1mval:\u001b[0m /kaggle/working/yolo_dataset/val... found 369 images in 15 classes ✅ \n\u001b[34m\u001b[1mtest:\u001b[0m None...\nOverriding model.yaml nc=80 with nc=15\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1      2784  ultralytics.nn.modules.conv.Conv             [3, 96, 3, 2]                 \n  1                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n  2                  -1  2    389760  ultralytics.nn.modules.block.C3k2            [192, 384, 2, True, 0.25]     \n  3                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n  4                  -1  2   1553664  ultralytics.nn.modules.block.C3k2            [384, 768, 2, True, 0.25]     \n  5                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n  6                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]           \n  7                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n  8                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]           \n  9                  -1  2   3264768  ultralytics.nn.modules.block.C2PSA           [768, 768, 2]                 \n 10                  -1  1   1004815  ultralytics.nn.modules.head.Classify         [768, 15]                     \nYOLO11x-cls summary: 176 layers, 28,375,279 parameters, 28,375,279 gradients, 111.0 GFLOPs\nTransferred 492/494 items from pretrained weights\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1307.0±444.7 MB/s, size: 47.1 KB)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/yolo_dataset/train... 6542 images, 0 corrupt: 100%|██████████| 6542/6542 [00:08<00:00, 730.86it/s] ","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5741.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5742.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5743.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5744.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5745.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5746.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5747.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5748.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5749.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5750.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5751.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5752.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5753.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5754.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5755.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5756.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5757.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5758.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5759.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5760.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5761.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5762.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5763.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5764.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5765.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5766.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5767.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5768.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5769.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5770.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5771.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5772.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5773.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5774.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5775.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5776.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5777.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5778.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5779.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5780.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5781.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5782.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5783.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5784.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5785.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5786.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5787.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5788.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0m/kaggle/working/yolo_dataset/train/13/img_5789.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/yolo_dataset/train.cache\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2190.4±1340.4 MB/s, size: 184.7 KB)\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/yolo_dataset/val... 369 images, 0 corrupt: 100%|██████████| 369/369 [00:00<00:00, 731.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/yolo_dataset/val/13/img_0324.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/yolo_dataset/val/13/img_0325.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/yolo_dataset/val/13/img_0326.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/yolo_dataset/val.cache\n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000526, momentum=0.9) with parameter groups 82 weight(decay=0.0), 83 weight(decay=0.0005), 83 bias(decay=0.0)\nImage sizes 256 train, 256 val\nUsing 0 dataloader workers\nLogging results to \u001b[1mruns/classify/train\u001b[0m\nStarting training for 50 epochs...\n\n      Epoch    GPU_mem       loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/50         0G      2.846         32        256:   0%|          | 1/205 [00:26<1:28:48, 26.12s/it]","output_type":"stream"},{"name":"stdout","text":"Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0.00/755k [00:00<?, ?B/s]\u001b[A\n100%|██████████| 755k/755k [00:00<00:00, 4.13MB/s]\u001b[A\n       1/50         0G      2.803         32        256:   1%|          | 2/205 [00:50<1:24:54, 25.10s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport numpy as np\nfrom ultralytics import YOLO\n\n# Load model\nmodel = YOLO('/kaggle/working/runs/classify/train4/weights/best.pt')\n\n# Paths\nval_image_dir = '/kaggle/input/smai-25-sec-a-project-phase-2-region-id-prediction/images_val/images_val'\nlabels_val_path = '/kaggle/input/smai-25-sec-a-project-phase-2-region-id-prediction/labels_val.csv'\n\n# Load label file\nlabels_df = pd.read_csv(labels_val_path)\nlabels_df['filename'] = labels_df['filename'].apply(lambda x: os.path.join(val_image_dir, x))\nprint(labels_df.head())\n\n# Create a mapping from YOLO class indices to Region_ID values\n# First, get the unique region IDs from your validation data\nunique_region_ids = sorted(labels_df['Region_ID'].unique())\nprint(f\"Unique Region IDs: {unique_region_ids}\")\n\n# Get the class names from your YAML file to understand YOLO's ordering\n# Or use this logic which shows the order YOLO uses\nclass_folders = ['0', '1', '10', '11', '12', '13', '14', '2', '3', '4', '5', '6', '7', '8', '9']\n# The actual indices YOLO assigns (alphabetical order)\nyolo_indices = list(range(len(class_folders)))\n# Map from YOLO index to class folder name\nidx_to_folder = {idx: folder for idx, folder in zip(yolo_indices, class_folders)}\nprint(f\"YOLO index to folder mapping: {idx_to_folder}\")\n\n# Create a mapping from YOLO prediction to actual Region_ID\n# If your folder names match the Region_IDs directly\nidx_to_region_id = {idx: int(folder) for idx, folder in idx_to_folder.items()}\nprint(f\"YOLO index to Region_ID mapping: {idx_to_region_id}\")\n\n# Predict on val images\nregion_ids = []\ncorrect_count = 0\ntotal_count = 0\n\nfor index, row in labels_df.iterrows():\n    img_path = row['filename']\n    true_region_id = row['Region_ID']\n    \n    result = model.predict(img_path, verbose=False)\n    pred_class_idx = int(result[0].probs.top1)  # Get top prediction index\n    \n    # Map the predicted class index to the correct Region_ID\n    pred_region_id = idx_to_region_id[pred_class_idx] + 1\n    region_ids.append(pred_region_id)\n    \n    # Track accuracy\n    if pred_region_id == true_region_id:\n        correct_count += 1\n    total_count += 1\n\nprint(f\"Validation accuracy: {correct_count/total_count:.4f}\")\n\n# Prepare validation part of submission\nval_df = pd.DataFrame({\n    'id': labels_df.index,\n    'Region_ID': region_ids,\n    # 'Real_RID': labels_df['Region_ID']\n})\n\n# Prepare test part (ID 369 to 737)\n# You may want to predict on test images properly instead of using dummy values\n# test_image_dir = '/kaggle/input/smai-25-sec-a-project-phase-2-region-id-prediction/images_test/images_test'\n# test_filenames = sorted(os.listdir(test_image_dir))\n\n# test_region_ids = []\n# for filename in test_filenames:\n#     img_path = os.path.join(test_image_dir, filename)\n#     result = model.predict(img_path, verbose=False)\n#     pred_class_idx = int(result[0].probs.top1)\n#     pred_region_id = idx_to_region_id[pred_class_idx]\n#     test_region_ids.append(pred_region_id)\nlenh = 369\ntest_df = pd.DataFrame({\n    'id': list(range(369, 369 + lenh)),\n    'Region_ID': [1] * lenh,\n    # 'Real_RID': [1] * lenh  # Placeholder\n})\n\n# Combine\nsubmission_df = pd.concat([val_df, test_df], ignore_index=True)\nprint(submission_df.head())\n\n# Save submission\nsubmission_filename = '2022102070.csv'\nsubmission_df.to_csv(submission_filename, index=False)\nprint(f\"Submission file saved as: {submission_filename}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-06T00:09:14.238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.init(\n    project=\"smai-project-regionID-classification\",  # Change as needed\n    name=\"upload-best-model\",\n    job_type=\"model-upload\"\n)\nartifact = wandb.Artifact(\n    name=\"yolo-best-classifier\",  # A name you'll use to refer to this model later\n    type=\"model\"\n)\nartifact = wandb.Artifact(\n    name = \"yolo-results-csv\",\n    type = \"results\"\n)\nartifact.add_file(\"/kaggle/working/runs/classify/train4/weights/best.pt\")  # or your renamed version\n\nwandb.log_artifact(artifact)\n\nwandb.finish()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-06T00:09:14.238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-06T00:09:14.238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# wandb.login()\n# wandb.init(\n#     project=\"smai-project-regionID-classification\",\n#     name=\"offline-efficientnet-partial-finetune-07-save-model\",\n#     settings=wandb.Settings(init_timeout=180)\n# )\n# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\n# # Load pretrained model\n# model_eb2 = models.efficientnet_b2(pretrained=True)\n# num_classes = 15\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Replace final layer in classifier\n# in_features = model_eb2.classifier[1].in_features\n# model_eb2.classifier[1] = nn.Linear(in_features, num_classes)\n# model_eb2 = model_eb2.to(device)\n\n# # Freeze early layers (first 70% of the model)\n# total_blocks = len(model_eb2.features)\n# freeze_blocks = int(total_blocks * 0.7)  # Freeze first 70% of feature blocks\n\n# # Freeze early layers\n# for i, block in enumerate(model_eb2.features):\n#     if i < freeze_blocks:\n#         for param in block.parameters():\n#             param.requires_grad = False\n#     else:\n#         for param in block.parameters():\n#             param.requires_grad = True\n\n# # Always train the classifier layers\n# for param in model_eb2.classifier.parameters():\n#     param.requires_grad = True\n\n# # Use a lower learning rate for the non-frozen layers compared to the classifier\n# optimizer = optim.Adam([\n#     {'params': model_eb2.features[freeze_blocks:].parameters(), 'lr': 0.0005},\n#     {'params': model_eb2.classifier.parameters(), 'lr': 0.001}\n# ])\n\n# # Define the learning rate scheduler\n# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n\n# criterion = nn.CrossEntropyLoss()\n# num_epochs = 40\n# best_val_loss = float('inf')\n# patience = 5  # Increased patience for better convergence\n# patience_counter = 0\n# best_model_state = None\n\n# from tqdm import tqdm\n\n# # Print trainable parameters to verify setup\n# trainable_params = sum(p.numel() for p in model_eb2.parameters() if p.requires_grad)\n# total_params = sum(p.numel() for p in model_eb2.parameters())\n# print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%} of total)\")\n\n# for epoch in range(num_epochs):\n#     print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n#     print(\"-\" * 30)\n    \n#     # ---------- Training Phase ----------\n#     model_eb2.train()\n#     train_loss = 0.0\n#     train_correct = 0\n#     train_total = 0\n    \n#     for images, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n#         images = images.to(device)\n#         labels = labels.to(device)\n        \n#         optimizer.zero_grad()\n#         outputs = model_eb2(images)\n#         loss = criterion(outputs, labels)\n#         loss.backward()\n#         optimizer.step()\n        \n#         train_loss += loss.item() * images.size(0)\n#         _, predicted = torch.max(outputs, 1)\n#         train_total += labels.size(0)\n#         train_correct += (predicted == labels).sum().item()\n    \n#     avg_train_loss = train_loss / train_total\n#     train_accuracy = 100 * train_correct / train_total\n    \n#     # ---------- Validation Phase ----------\n#     model_eb2.eval()\n#     val_loss = 0.0\n#     val_correct = 0\n#     val_total = 0\n    \n#     with torch.no_grad():\n#         for images, labels in tqdm(validation_loader, desc=\"Validation\", leave=False):\n#             images = images.to(device)\n#             labels = labels.to(device)\n            \n#             outputs = model_eb2(images)\n#             loss = criterion(outputs, labels)\n            \n#             val_loss += loss.item() * images.size(0)\n#             _, predicted = torch.max(outputs, 1)\n#             val_total += labels.size(0)\n#             val_correct += (predicted == labels).sum().item()\n    \n#     avg_val_loss = val_loss / val_total\n#     val_accuracy = 100 * val_correct / val_total\n    \n#     # Update learning rate scheduler\n#     scheduler.step(avg_val_loss)\n    \n#     # ---------- Epoch Summary ----------\n#     print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%\")\n#     print(f\"Val   Loss: {avg_val_loss:.4f} | Val   Acc: {val_accuracy:.2f}%\")\n    \n#     wandb.log({\n#         'avg_val_loss': avg_val_loss,\n#         'val_accuracy': val_accuracy,\n#         'avg_train_loss': avg_train_loss,\n#         'train_accuracy': train_accuracy,\n#         'epoch': epoch,\n#         'learning_rate': optimizer.param_groups[0]['lr']\n#     })\n    \n#     if avg_val_loss < best_val_loss:\n#         best_val_loss = avg_val_loss\n#         patience_counter = 0\n#         best_model_state = model_eb2.state_dict()\n#         print(\"✅ Validation loss improved. Saving model.\")\n#     else:\n#         patience_counter += 1\n#         print(f\"⚠️ No improvement in val loss. Patience {patience_counter}/{patience}\")\n#         if patience_counter >= patience:\n#             print(\"⛔ Early stopping triggered.\")\n#             break\n\n# torch.save(best_model_state, 'best_model_partial_finetune_eb2.pth')\n\n\n# artifact = wandb.Artifact('efficientnet-best-model', type='model')\n# artifact.add_file('best_model_partial_finetune_eb2.pth')\n# wandb.log_artifact(artifact)\n\n# import pandas as pd\n# import os\n# import numpy as np\n# import torch\n# import torch.nn as nn\n# from torchvision import models, transforms\n# from PIL import Image\n# import wandb\n\n# # Initialize wandb\n# wandb.init(\n#     project=\"smai-project-regionID-classification\",\n#     name=\"efficientnet-inference\",\n# )\n\n# # Define image transformations (must match what was used during training)\n# transform = transforms.Compose([\n#     transforms.Resize(224),\n#     transforms.ToTensor(),\n# ])\n\n# # Paths\n# val_image_dir = '/kaggle/input/smai-25-sec-a-project-phase-2-region-id-prediction/images_val/images_val'\n# labels_val_path = '/kaggle/input/smai-25-sec-a-project-phase-2-region-id-prediction/labels_val.csv'\n# test_image_dir = '/kaggle/input/smai-25-sec-a-project-phase-2-region-id-prediction/images_test/images_test'\n\n# # Load label file\n# labels_df = pd.read_csv(labels_val_path)\n# labels_df['filename'] = labels_df['filename'].apply(lambda x: os.path.join(val_image_dir, x))\n# print(labels_df.head())\n\n# # Load the EfficientNet model\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model_eb2 = models.efficientnet_b2(pretrained=False)\n# num_classes = 15\n# in_features = model_eb2.classifier[1].in_features\n# model_eb2.classifier[1] = nn.Linear(in_features, num_classes)\n\n# # Load the saved model weights\n# model_eb2.load_state_dict(torch.load('best_model_partial_finetune_eb2.pth'))\n# model_eb2 = model_eb2.to(device)\n# model_eb2.eval()\n\n# print(\"Model loaded successfully\")\n\n# def predict_region_id(img_path, model):\n#     try:\n#         image = Image.open(img_path).convert('RGB')\n#         image_tensor = transform(image).unsqueeze(0).to(device)\n        \n#         with torch.no_grad():\n#             outputs = model(image_tensor)\n#             _, predicted = torch.max(outputs, 1)\n            \n#         pred_region_id = predicted.item() + 1\n#         return pred_region_id\n#     except Exception as e:\n#         print(f\"Error processing {img_path}: {e}\")\n#         return 1  # Default value in case of error\n\n# # Predict on validation images\n# region_ids = []\n# correct_count = 0\n# total_count = 0\n\n# print(\"Processing validation images...\")\n# for index, row in labels_df.iterrows():\n#     img_path = row['filename']\n#     true_region_id = row['Region_ID']\n    \n#     pred_region_id = predict_region_id(img_path, model_eb2)\n#     region_ids.append(pred_region_id)\n    \n#     # Track accuracy\n#     if pred_region_id == true_region_id:\n#         correct_count += 1\n#     total_count += 1\n    \n#     if index % 50 == 0:\n#         print(f\"Processed {index} validation images\")\n\n# validation_accuracy = correct_count/total_count\n# print(f\"Validation accuracy: {validation_accuracy:.4f}\")\n\n# # Prepare validation part of submission\n# val_df = pd.DataFrame({\n#     'id': labels_df.index,\n#     'Region_ID': region_ids\n# })\n# val_df2 = pd.DataFrame({\n#     'id': labels_df.index,\n#     'Region_ID': region_ids,\n#     'Real_RID': labels_df['Region_ID']\n# })\n\n# # Process test images\n# # print(\"Processing test images...\")\n# # test_filenames = sorted(os.listdir(test_image_dir))\n# # test_region_ids = []\n\n# # for filename in test_filenames:\n# #     img_path = os.path.join(test_image_dir, filename)\n# #     pred_region_id = predict_region_id(img_path, model_eb2)\n# #     test_region_ids.append(pred_region_id)\n    \n# #     if len(test_region_ids) % 50 == 0:\n# #         print(f\"Processed {len(test_region_ids)} test images\")\n# lenh = 369\n# # Prepare test part of submission\n# test_df = pd.DataFrame({\n#     'id': list(range(369, 369 + lenh)),\n#     'Region_ID': [1]*lenh\n# })\n# test_df2 = pd.DataFrame({\n#     'id': list(range(369, 369 + lenh)),\n#     'Region_ID': [1]*lenh,\n#     'Real_RID':[1]*lenh\n# })\n# # Combine validation and test predictions\n# submission_df = pd.concat([val_df, test_df], ignore_index=True)\n# submission_df2 = pd.concat([val_df2, test_df2], ignore_index=True)\n# print(submission_df.head())\n\n# # Save submission file\n# submission_filename = '2022102070.csv'\n# submission_df.to_csv(submission_filename, index=False)\n# print(f\"Submission file saved as: {submission_filename}\")\n\n# submission_filename2 = '2022102070_withcorrect.csv'\n# submission_df2.to_csv(submission_filename2, index=False)\n# print(f\"Submission file saved as: {submission_filename2}\")\n\n# # Log the CSV file to wandb\n# artifact = wandb.Artifact('prediction_results', type='dataset')\n# artifact.add_file(submission_filename)\n# wandb.log_artifact(artifact)\n# artifact = wandb.Artifact('prediction_results_with_real', type='dataset')\n# artifact.add_file(submission_filename2)\n# wandb.log_artifact(artifact)\n\n# # Log the validation accuracy to wandb\n# wandb.log({\"validation_accuracy\": validation_accuracy})","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-06T00:09:14.238Z"}},"outputs":[],"execution_count":null}]}