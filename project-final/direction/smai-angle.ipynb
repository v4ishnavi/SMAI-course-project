{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":97830,"databundleVersionId":11787219,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"data_path = \"/kaggle/input/smai-s-25-section-a-project-phase-2\"\nimport os\nimport pandas as pd\nimport numpy as np\nimages_train_path = os.path.join(data_path, 'images_train/images_train')\nimages_val_path = os.path.join(data_path, 'images_val/images_val')\nlabels_train = os.path.join(data_path, 'labels_train.csv')\nlabels_val = os.path.join(data_path, 'labels_val.csv')\n\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(labels_train)\n# df['timestamp'] = pd.to_datetime(df['timestamp'], format='%H:%M').dt.time\n\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T11:23:32.508995Z","iopub.execute_input":"2025-05-03T11:23:32.509278Z","iopub.status.idle":"2025-05-03T11:23:33.447446Z","shell.execute_reply.started":"2025-05-03T11:23:32.509254Z","shell.execute_reply":"2025-05-03T11:23:33.446644Z"}},"outputs":[{"name":"stdout","text":"       filename timestamp  latitude  longitude  angle  Region_ID\n0  img_0000.jpg     15:03    219698     144782    133          2\n1  img_0001.jpg     15:05    219844     144621    312          2\n2  img_0002.jpg     15:05    219844     144621    359          2\n3  img_0003.jpg     17:11    219514     145016    131          2\n4  img_0004.jpg     17:00    220182     144211     45          2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nimport cv2\nimport os\nimage = cv2.imread(os.path.join(images_train_path, df['filename'][0]))\nprint(image.shape)\n\n# in the df change all the file names to full path \ndf['filename'] = df['filename'].apply(lambda x: os.path.join(images_train_path, x))\ndf.head()\n\n# some data cleaning, if angle > 360 remove\ndf = df[df['angle'] < 361]\ndf.head()\ndf['angle_rad'] = np.deg2rad(df['angle'])\ndf['angle_sin'] = np.sin(df['angle_rad'])\ndf['angle_cos'] = np.cos(df['angle_rad'])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T11:23:41.590732Z","iopub.execute_input":"2025-05-03T11:23:41.591488Z","iopub.status.idle":"2025-05-03T11:23:41.979059Z","shell.execute_reply.started":"2025-05-03T11:23:41.591460Z","shell.execute_reply":"2025-05-03T11:23:41.978296Z"}},"outputs":[{"name":"stdout","text":"(256, 256, 3)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"validation_df = pd.read_csv(labels_val)\nvalidation_df['filename'] = validation_df['filename'].apply(lambda x: os.path.join(images_val_path, x))\nvalidation_df.head()\n\nvalidation_df['angle_rad'] = np.deg2rad(validation_df['angle'])\nvalidation_df['angle_sin'] = np.sin(validation_df['angle_rad'])\nvalidation_df['angle_cos'] = np.cos(validation_df['angle_rad'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T11:23:45.987045Z","iopub.execute_input":"2025-05-03T11:23:45.987737Z","iopub.status.idle":"2025-05-03T11:23:45.999765Z","shell.execute_reply.started":"2025-05-03T11:23:45.987708Z","shell.execute_reply":"2025-05-03T11:23:45.999199Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from torchvision import transforms\n\ntrain_transforms = transforms.Compose([\n    # transforms.RandomResizedCrop(256, scale=(0.8,1.0)),\n    transforms.Resize((224, 224)),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n])\n# can add a randomposterize \n# random adjust sharpness \n# random autocontrast \n\ntrain_transforms_2 = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomAutocontrast(),\n    transforms.RandomAdjustSharpness(sharpness_factor=2),\n    transforms.RandomPosterize(bits=4),\n    transforms.ToTensor()\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T11:23:48.562797Z","iopub.execute_input":"2025-05-03T11:23:48.563052Z","iopub.status.idle":"2025-05-03T11:23:54.971131Z","shell.execute_reply.started":"2025-05-03T11:23:48.563030Z","shell.execute_reply":"2025-05-03T11:23:54.970568Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T11:23:54.972215Z","iopub.execute_input":"2025-05-03T11:23:54.972637Z","iopub.status.idle":"2025-05-03T11:23:54.976326Z","shell.execute_reply.started":"2025-05-03T11:23:54.972613Z","shell.execute_reply":"2025-05-03T11:23:54.975595Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, df, transform1=None, transform2 = None, validation=False):\n        self.df = df\n        self.transform1 = transform1\n        self.transform2 = transform2 \n        self.triple_len = len(df) * 3\n        self.validation = validation\n    \n\n    def __len__(self):\n        if self.validation:\n            return len(self.df)\n        else:\n            return self.triple_len\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx % len(self.df)]\n        img = Image.open(f\"{row.filename}\").convert('RGB')\n        img = img.resize((224, 224))\n\n        if self.validation or idx % len(self.df) == 0:\n            transformed_img = transforms.ToTensor()(img)\n        elif idx > len(self.df) and idx < 2 * len(self.df):\n            transformed_img = self.transform1(img) if self.transform1 else transforms.ToTensor()(img)\n        else:\n            transformed_img = self.transform2(img) if self.transform2 else transforms.ToTensor()(img)\n\n        label = row['angle']\n        return transformed_img, label\n\n\ntrain_dataset = MyDataset(df, transform1=train_transforms, transform2 = train_transforms_2) \nprint(len(train_dataset))\n# img_sample, label = train_dataset[len(df) + 4]\n# print(img_sample.shape)\n# print(label)\nvalidation_dataset = MyDataset(validation_df, validation=True)\n\nprint(len(validation_dataset))\n# img_sample, label = validation_dataset[0]\n# print(img_sample.shape)\n# print(label)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T11:23:57.256826Z","iopub.execute_input":"2025-05-03T11:23:57.257474Z","iopub.status.idle":"2025-05-03T11:23:57.265449Z","shell.execute_reply.started":"2025-05-03T11:23:57.257449Z","shell.execute_reply":"2025-05-03T11:23:57.264597Z"}},"outputs":[{"name":"stdout","text":"19620\n369\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=32, shuffle=False)\n# device = \nprint(len(train_loader))\nprint(len(validation_loader))\n# print(device)\nprint(train_loader.dataset[0][0].shape)\n# sample_image, sample_label = train_loader.dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T11:24:01.127172Z","iopub.execute_input":"2025-05-03T11:24:01.127817Z","iopub.status.idle":"2025-05-03T11:24:01.210806Z","shell.execute_reply.started":"2025-05-03T11:24:01.127786Z","shell.execute_reply":"2025-05-03T11:24:01.210034Z"}},"outputs":[{"name":"stdout","text":"614\n12\ntorch.Size([3, 224, 224])\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim\nimport os\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T11:24:04.161754Z","iopub.execute_input":"2025-05-03T11:24:04.162030Z","iopub.status.idle":"2025-05-03T11:24:04.165862Z","shell.execute_reply.started":"2025-05-03T11:24:04.162007Z","shell.execute_reply":"2025-05-03T11:24:04.165288Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb-key\")\n\nwandb.login(key = secret_value_0)\n\nwandb.init(\n    project = \"smai-project-angleID\",\n    name = 'method2-efficientnetb2-angle-regression'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T11:24:08.069148Z","iopub.execute_input":"2025-05-03T11:24:08.069436Z","iopub.status.idle":"2025-05-03T11:24:23.343531Z","shell.execute_reply.started":"2025-05-03T11:24:08.069414Z","shell.execute_reply":"2025-05-03T11:24:23.342646Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mv4ishnavi\u001b[0m (\u001b[33mv4ishnavi-iiit-hyderabad\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250503_112416-ofo6lsw6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/v4ishnavi-iiit-hyderabad/smai-project-angleID/runs/ofo6lsw6' target=\"_blank\">method2-efficientnetb2-angle-regression</a></strong> to <a href='https://wandb.ai/v4ishnavi-iiit-hyderabad/smai-project-angleID' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/v4ishnavi-iiit-hyderabad/smai-project-angleID' target=\"_blank\">https://wandb.ai/v4ishnavi-iiit-hyderabad/smai-project-angleID</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/v4ishnavi-iiit-hyderabad/smai-project-angleID/runs/ofo6lsw6' target=\"_blank\">https://wandb.ai/v4ishnavi-iiit-hyderabad/smai-project-angleID/runs/ofo6lsw6</a>"},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/v4ishnavi-iiit-hyderabad/smai-project-angleID/runs/ofo6lsw6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7855cac8b8d0>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"0\"\n\nmodel_eb2 = models.efficientnet_b2(pretrained=True)\nin_features = model_eb2.classifier[1].in_features\nmodel_eb2.classifier[1] = nn.Linear(in_features, 2)  \nmodel_eb2 = model_eb2.to(device)\n\ntotal_blocks = len(model_eb2.features)\nfreeze_blocks = int(total_blocks * 0.7)\n\nfor i, block in enumerate(model_eb2.features):\n    if i < freeze_blocks:\n        for param in block.parameters():\n            param.requires_grad = False\n    else:\n        for param in block.parameters():\n            param.requires_grad = True\n\nfor param in model_eb2.classifier.parameters():\n    param.requires_grad = True\n\noptimizer = optim.Adam([\n    {'params': model_eb2.features[freeze_blocks:].parameters(), 'lr': 0.0005},\n    {'params': model_eb2.classifier.parameters(), 'lr': 0.001}\n])\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n\ncriterion = nn.MSELoss()\n\nnum_epochs = 40\nbest_val_loss = float('inf')\npatience = 5\npatience_counter = 0\nbest_model_state = None\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T11:24:23.344719Z","iopub.execute_input":"2025-05-03T11:24:23.344952Z","iopub.status.idle":"2025-05-03T11:24:24.205205Z","shell.execute_reply.started":"2025-05-03T11:24:23.344923Z","shell.execute_reply":"2025-05-03T11:24:24.204448Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B2_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-c35c1473.pth\n100%|██████████| 35.2M/35.2M [00:00<00:00, 166MB/s] \n/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Helper functions\ndef angle_to_sincos(angles):\n    \"\"\"Convert angles in degrees to sine and cosine components\"\"\"\n    angles = angles.float()  # Make sure we're working with float tensors for math operations\n    angles_rad = torch.deg2rad(angles)\n    sin_values = torch.sin(angles_rad)\n    cos_values = torch.cos(angles_rad)\n    return torch.cat([sin_values, cos_values], dim=1)\n\ndef sincos_to_angle(sin_cos):\n    \"\"\"Convert sine and cosine components back to angles in degrees (as integers)\"\"\"\n    sin, cos = sin_cos[:, 0], sin_cos[:, 1]\n    angles_rad = torch.atan2(sin, cos)\n    angles_deg = torch.rad2deg(angles_rad)\n    angles_int = torch.round((angles_deg + 360) % 360).int()  # Round to nearest integer\n    return angles_int\n\ndef calculate_maae(pred_angles, true_angles):\n    \"\"\"Calculate Mean Absolute Angular Error\"\"\"\n    if isinstance(pred_angles, torch.Tensor):\n        pred_angles = pred_angles.detach().cpu().numpy()\n    if isinstance(true_angles, torch.Tensor):\n        true_angles = true_angles.detach().cpu().numpy()\n    diff = np.abs(pred_angles - true_angles)\n    angular_diff = np.minimum(diff, 360 - diff)\n    return np.mean(angular_diff)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T11:24:24.206003Z","iopub.execute_input":"2025-05-03T11:24:24.206302Z","iopub.status.idle":"2025-05-03T11:24:24.213388Z","shell.execute_reply.started":"2025-05-03T11:24:24.206278Z","shell.execute_reply":"2025-05-03T11:24:24.212617Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"trainable_params = sum(p.numel() for p in model_eb2.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model_eb2.parameters())\nprint(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%} of total)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T11:24:29.515046Z","iopub.execute_input":"2025-05-03T11:24:29.515616Z","iopub.status.idle":"2025-05-03T11:24:29.526117Z","shell.execute_reply.started":"2025-05-03T11:24:29.515590Z","shell.execute_reply":"2025-05-03T11:24:29.525362Z"}},"outputs":[{"name":"stdout","text":"Trainable parameters: 6,285,948 (81.60% of total)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(\"-\" * 10)\n\n    # Training phase\n    model_eb2.train()\n    train_loss = 0.0\n    angle_preds, angle_truths = [], []\n    \n    for images, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n        images = images.to(device)\n        angles = labels.to(device).float().view(-1, 1)  # Original angles\n        \n        # Convert angles to sin/cos targets\n        sincos_targets = angle_to_sincos(angles)\n        \n        optimizer.zero_grad()\n        sincos_outputs = model_eb2(images)  # Model predicts sin and cos directly\n        \n                # Calculate MSE loss between predicted and target sin/cos values\n        loss = criterion(sincos_outputs, sincos_targets)\n        \n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * images.size(0)\n        \n        # Convert sin/cos outputs back to angles for MAAE calculation\n        predicted_angles = sincos_to_angle(sincos_outputs).view(-1, 1)\n        \n        angle_preds.append(predicted_angles.detach())\n        angle_truths.append(angles.detach())\n\n    train_total = len(train_loader.dataset)\n    avg_train_loss = train_loss / train_total\n    train_maae = calculate_maae(torch.cat(angle_preds), torch.cat(angle_truths))\n\n    # Validation phase\n    model_eb2.eval()\n    val_loss = 0.0\n    val_angle_preds, val_angle_truths = [], []\n\n    with torch.no_grad():\n        for images, labels in tqdm(validation_loader, desc=\"Validation\", leave=False):\n            images = images.to(device)\n            angles = labels.to(device).float().view(-1, 1)\n            \n            # Convert angles to sin/cos targets\n            sincos_targets = angle_to_sincos(angles)\n            \n            sincos_outputs = model_eb2(images)\n            loss = criterion(sincos_outputs, sincos_targets)\n            \n            val_loss += loss.item() * images.size(0)\n            \n            # Convert sin/cos outputs back to angles for MAAE calculation\n            predicted_angles = sincos_to_angle(sincos_outputs).view(-1, 1)\n            \n            val_angle_preds.append(predicted_angles)\n            val_angle_truths.append(angles)\n\n    val_total = len(validation_loader.dataset)\n    avg_val_loss = val_loss / val_total\n    val_maae = calculate_maae(torch.cat(val_angle_preds), torch.cat(val_angle_truths))\n\n    # Update learning rate scheduler\n    scheduler.step(avg_val_loss)\n\n    # Print epoch results\n    print(f\"Train Loss: {avg_train_loss:.4f} | Train MAAE: {train_maae:.2f}\")\n    print(f\"Val   Loss: {avg_val_loss:.4f} | Val   MAAE: {val_maae:.2f}\")\n\n    # Log metrics to WandB\n    wandb.log({\n        'avg_train_loss': avg_train_loss,\n        'avg_val_loss': avg_val_loss,\n        'train_maae': train_maae,\n        'val_maae': val_maae,\n        'epoch': epoch,\n        'learning_rate': optimizer.param_groups[0]['lr']\n    })\n\n    # Check for improvement and save best model\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        best_model_state = model_eb2.state_dict()\n        print(\"✅ Validation loss improved. Saving model.\")\n    else:\n        patience_counter += 1\n        print(f\"⚠️ No improvement in val loss. Patience {patience_counter}/{patience}\")\n        if patience_counter >= patience:\n            print(\"⛔ Early stopping triggered.\")\n            break\n\n# Save the best model\ntorch.save(best_model_state, 'best_model_sincos_eb2.pth')\n\n# Log artifact to WandB\nartifact = wandb.Artifact('efficientnet-sincos-model', type='model')\nartifact.add_file('best_model_sincos_eb2.pth')\nwandb.log_artifact(artifact)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T11:24:32.201164Z","iopub.execute_input":"2025-05-03T11:24:32.201771Z","iopub.status.idle":"2025-05-03T13:16:26.918198Z","shell.execute_reply.started":"2025-05-03T11:24:32.201748Z","shell.execute_reply":"2025-05-03T13:16:26.917557Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3098 | Train MAAE: 46.08\nVal   Loss: 0.3264 | Val   MAAE: 46.10\n✅ Validation loss improved. Saving model.\nEpoch 2/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1218 | Train MAAE: 21.15\nVal   Loss: 0.2624 | Val   MAAE: 38.96\n✅ Validation loss improved. Saving model.\nEpoch 3/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0783 | Train MAAE: 16.26\nVal   Loss: 0.2392 | Val   MAAE: 36.21\n✅ Validation loss improved. Saving model.\nEpoch 4/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0582 | Train MAAE: 13.66\nVal   Loss: 0.2338 | Val   MAAE: 34.92\n✅ Validation loss improved. Saving model.\nEpoch 5/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0553 | Train MAAE: 13.28\nVal   Loss: 0.2372 | Val   MAAE: 34.20\n⚠️ No improvement in val loss. Patience 1/5\nEpoch 6/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0442 | Train MAAE: 11.64\nVal   Loss: 0.2142 | Val   MAAE: 32.27\n✅ Validation loss improved. Saving model.\nEpoch 7/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0423 | Train MAAE: 11.27\nVal   Loss: 0.2473 | Val   MAAE: 35.54\n⚠️ No improvement in val loss. Patience 1/5\nEpoch 8/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0397 | Train MAAE: 10.87\nVal   Loss: 0.2306 | Val   MAAE: 35.16\n⚠️ No improvement in val loss. Patience 2/5\nEpoch 9/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0379 | Train MAAE: 10.58\nVal   Loss: 0.2102 | Val   MAAE: 31.52\n✅ Validation loss improved. Saving model.\nEpoch 10/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0358 | Train MAAE: 10.18\nVal   Loss: 0.2340 | Val   MAAE: 35.59\n⚠️ No improvement in val loss. Patience 1/5\nEpoch 11/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0406 | Train MAAE: 10.85\nVal   Loss: 0.2203 | Val   MAAE: 32.47\n⚠️ No improvement in val loss. Patience 2/5\nEpoch 12/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0256 | Train MAAE: 8.54\nVal   Loss: 0.2235 | Val   MAAE: 32.41\n⚠️ No improvement in val loss. Patience 3/5\nEpoch 13/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0166 | Train MAAE: 6.75\nVal   Loss: 0.2059 | Val   MAAE: 30.88\n✅ Validation loss improved. Saving model.\nEpoch 14/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0128 | Train MAAE: 5.90\nVal   Loss: 0.1962 | Val   MAAE: 30.28\n✅ Validation loss improved. Saving model.\nEpoch 15/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0122 | Train MAAE: 5.69\nVal   Loss: 0.1847 | Val   MAAE: 28.65\n✅ Validation loss improved. Saving model.\nEpoch 16/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0117 | Train MAAE: 5.60\nVal   Loss: 0.1892 | Val   MAAE: 28.28\n⚠️ No improvement in val loss. Patience 1/5\nEpoch 17/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0129 | Train MAAE: 5.86\nVal   Loss: 0.1905 | Val   MAAE: 28.90\n⚠️ No improvement in val loss. Patience 2/5\nEpoch 18/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0120 | Train MAAE: 5.67\nVal   Loss: 0.1948 | Val   MAAE: 29.37\n⚠️ No improvement in val loss. Patience 3/5\nEpoch 19/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0094 | Train MAAE: 4.99\nVal   Loss: 0.1810 | Val   MAAE: 27.38\n✅ Validation loss improved. Saving model.\nEpoch 20/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0083 | Train MAAE: 4.65\nVal   Loss: 0.1874 | Val   MAAE: 27.99\n⚠️ No improvement in val loss. Patience 1/5\nEpoch 21/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0074 | Train MAAE: 4.39\nVal   Loss: 0.1876 | Val   MAAE: 27.86\n⚠️ No improvement in val loss. Patience 2/5\nEpoch 22/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0068 | Train MAAE: 4.24\nVal   Loss: 0.1816 | Val   MAAE: 27.46\n⚠️ No improvement in val loss. Patience 3/5\nEpoch 23/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0058 | Train MAAE: 3.90\nVal   Loss: 0.1792 | Val   MAAE: 27.31\n✅ Validation loss improved. Saving model.\nEpoch 24/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0057 | Train MAAE: 3.84\nVal   Loss: 0.1787 | Val   MAAE: 27.29\n✅ Validation loss improved. Saving model.\nEpoch 25/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0055 | Train MAAE: 3.74\nVal   Loss: 0.1787 | Val   MAAE: 26.93\n✅ Validation loss improved. Saving model.\nEpoch 26/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0055 | Train MAAE: 3.75\nVal   Loss: 0.1793 | Val   MAAE: 27.30\n⚠️ No improvement in val loss. Patience 1/5\nEpoch 27/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0053 | Train MAAE: 3.65\nVal   Loss: 0.1734 | Val   MAAE: 27.14\n✅ Validation loss improved. Saving model.\nEpoch 28/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0050 | Train MAAE: 3.59\nVal   Loss: 0.1692 | Val   MAAE: 26.63\n✅ Validation loss improved. Saving model.\nEpoch 29/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0051 | Train MAAE: 3.59\nVal   Loss: 0.1751 | Val   MAAE: 27.35\n⚠️ No improvement in val loss. Patience 1/5\nEpoch 30/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0046 | Train MAAE: 3.50\nVal   Loss: 0.1707 | Val   MAAE: 26.51\n⚠️ No improvement in val loss. Patience 2/5\nEpoch 31/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0045 | Train MAAE: 3.38\nVal   Loss: 0.1726 | Val   MAAE: 27.02\n⚠️ No improvement in val loss. Patience 3/5\nEpoch 32/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0046 | Train MAAE: 3.39\nVal   Loss: 0.1713 | Val   MAAE: 26.94\n⚠️ No improvement in val loss. Patience 4/5\nEpoch 33/40\n----------\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0042 | Train MAAE: 3.28\nVal   Loss: 0.1728 | Val   MAAE: 27.56\n⚠️ No improvement in val loss. Patience 5/5\n⛔ Early stopping triggered.\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<Artifact efficientnet-sincos-model>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# making the csv file\n# validation transform \ntransform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n])\n\nlabels_df = pd.read_csv(labels_val)\nlabels_df['filename'] = labels_df['filename'].apply(lambda x: os.path.join(images_val_path, x))\nprint(labels_df.head())\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_eb2 = models.efficientnet_b2(pretrained = True)\nin_features = model_eb2.classifier[1].in_features\nmodel_eb2.classifier[1] = nn.Linear(in_features, 2)\n\nmodel_eb2.load_state_dict(torch.load('best_model_sincos_eb2.pth'))\nmodel_eb2 = model_eb2.to(device)\nmodel_eb2.eval()\n\nprint(\"model loading done\")\n\nangles = []\ntrue_angles = []\n\nfor idx, row in labels_df.iterrows():\n    img = Image.open(row['filename']).convert('RGB')\n    img_tensor = transform(img).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        output = model_eb2(img_tensor)\n        norm = torch.sqrt(output[:, 0]**2 + output[:, 1]**2).view(-1, 1)\n        normalized_output = output / (norm + 1e-8)\n        pred_angle = sincos_to_angle(normalized_output).item()\n\n    angles.append(pred_angle)\n    true_angles.append(row['angle'])\n\n    if idx % 50 == 0:\n        print(f\"Processed {idx} / {len(labels_df)}\")\ndef calculate_maae2(pred_angles, true_angles):\n    pred_angles = np.array(pred_angles)\n    true_angles = np.array(true_angles)\n    diff = np.abs(pred_angles - true_angles)\n    angular_diff = np.minimum(diff, 360 - diff)\n    return np.mean(angular_diff)\n# --- Compute final MAAE ---\nmaae_val = calculate_maae2(angles, true_angles)\nprint(f\"📏 Validation MAAE: {maae_val:.2f}°\")\n\n\nval_df = pd.DataFrame({\n    'id': labels_df.index,\n    'angle': angles\n})\nval_df2 = pd.DataFrame({\n    'id': labels_df.index,\n    'angle': angles,\n    'Real_Angle': labels_df['angle']\n})\n\nlenh = 369\ntest_df = pd.DataFrame({\n    'id': list(range(369, 369 + lenh)),\n    'angle': [0]*lenh\n})\ntest_df2 = pd.DataFrame({\n    'id': list(range(369, 369 + lenh)),\n    'angle': [0]*lenh,\n    'Real_Angle': [0]*lenh\n})\n\n# --- Combine and Save ---\nsubmission_df = pd.concat([val_df, test_df], ignore_index=True)\nsubmission_df2 = pd.concat([val_df2, test_df2], ignore_index=True)\n\nsubmission_filename = '2022102070.csv'\nsubmission_filename2 = '2022102070_withcorrect.csv'\n\nsubmission_df.to_csv(submission_filename, index=False)\nsubmission_df2.to_csv(submission_filename2, index=False)\n\nprint(f\"✅ Submission saved as: {submission_filename}\")\nprint(f\"✅ Submission with real angles saved as: {submission_filename2}\")\n\nartifact = wandb.Artifact('prediction_results', type='dataset')\nartifact.add_file(submission_filename)\nwandb.log_artifact(artifact)\n\nartifact = wandb.Artifact('prediction_results_with_real', type='dataset')\nartifact.add_file(submission_filename2)\nwandb.log_artifact(artifact)\n\nwandb.log({\"validation_maae\": maae_val})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:24:12.839787Z","iopub.execute_input":"2025-05-03T13:24:12.840057Z","iopub.status.idle":"2025-05-03T13:24:19.844006Z","shell.execute_reply.started":"2025-05-03T13:24:12.840036Z","shell.execute_reply":"2025-05-03T13:24:19.843221Z"}},"outputs":[{"name":"stdout","text":"                                            filename timestamp  latitude  \\\n0  /kaggle/input/smai-s-25-section-a-project-phas...     15:04    219698   \n1  /kaggle/input/smai-s-25-section-a-project-phas...     17:00    220182   \n2  /kaggle/input/smai-s-25-section-a-project-phas...     17:00    220182   \n3  /kaggle/input/smai-s-25-section-a-project-phas...     15:11    220195   \n4  /kaggle/input/smai-s-25-section-a-project-phas...     17:35    220437   \n\n   longitude  angle  Region_ID  \n0     144782    311          2  \n1     144211     89          2  \n2     144211    177          2  \n3     141942    301         12  \n4     142673    323         12  \n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B2_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/tmp/ipykernel_31/4286009153.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_eb2.load_state_dict(torch.load('best_model_sincos_eb2.pth'))\n","output_type":"stream"},{"name":"stdout","text":"model loading done\nProcessed 0 / 369\nProcessed 50 / 369\nProcessed 100 / 369\nProcessed 150 / 369\nProcessed 200 / 369\nProcessed 250 / 369\nProcessed 300 / 369\nProcessed 350 / 369\n📏 Validation MAAE: 27.64°\n✅ Submission saved as: 2022102070.csv\n✅ Submission with real angles saved as: 2022102070_withcorrect.csv\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mmethod2-efficientnetb2-angle-regression\u001b[0m at: \u001b[34mhttps://wandb.ai/v4ishnavi-iiit-hyderabad/smai-project-angleID/runs/ofo6lsw6\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250503_112416-ofo6lsw6/logs\u001b[0m\n","output_type":"stream"}],"execution_count":16}]}